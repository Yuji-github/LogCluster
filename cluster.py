import random
import numpy as np
import collections
import math
from hdbscan import HDBSCAN
from sklearn.cluster import DBSCAN, MeanShift, AffinityPropagation, estimate_bandwidth
from sklearn.decomposition import FastICA
from sentence_transformers import SentenceTransformer

bert_model = SentenceTransformer("google/bert_uncased_L-2_H-128_A-2")


def clustering(line: np.ndarray, rate: float, num_cluster: int, cluster: str) -> list:
    """cluster messages by using the given clustering algorithm

    :param line: input message (generated by log_loader)
    :param rate: rate of clustering
    :param num_cluster: number of clusters
    :param cluster: name of clustering algorithm
    :return: sample of cluster index
    """
    # at most 100,000 lines
    sample_rate = float(rate)  # default 10%

    # sample lines for parse tree in total
    total_sample = math.floor(len(line) * sample_rate)  # 100,000 * rate
    if (num_cluster * sample_rate) > num_cluster:  # more than the given
        total_sample = math.floor(len(line) * 0.1)

    # shuffling the original data set
    shuffle_line = line.copy()  # to shuffle
    np.random.shuffle(shuffle_line)  # shuffle copy line

    # other items we need
    round = 1  # each round get sample values
    begin, end = 0, total_sample
    round_sample = total_sample // num_cluster
    sample_index = []

    while round <= num_cluster:
        # converting values for cluster with Bert
        data = bert_model.encode(
            shuffle_line[begin:end], show_progress_bar=False
        )  # convert to vector

        try:
            print("Start FastICA, target dimension: {0}".format(64))
            transformers = FastICA(n_components=64)
            data = transformers.fit_transform(data)
        except ValueError:
            print("Captured Value Error by Inf or Nan")
            print("This Round Does Not Use FastICA")

        # creating clusters
        if cluster.lower() == "dbscan":
            clustering = DBSCAN(eps=0.5, n_jobs=-1).fit_predict(data)
        elif cluster.lower() == "hdbscan":
            clustering = HDBSCAN(algorithm="best", metric="euclidean").fit_predict(
                np.asarray(data, dtype=np.float64)
            )
        elif cluster.lower() == "mean-shift":
            bandwidth = estimate_bandwidth(data)
            clustering = MeanShift(bandwidth=bandwidth).fit_predict(
                np.asarray(data, dtype=np.float64)
            )
        elif cluster.lower() == "affinity-propagation":
            clustering = AffinityPropagation(
                preference=-50, random_state=0
            ).fit_predict(data)
        else:
            print(f"Unknown Cluster: Using HDBSCAN instead of {cluster}")
            clustering = HDBSCAN(algorithm="best", metric="euclidean").fit_predict(
                np.asarray(data, dtype=np.float64)
            )

        # counting by cluster groups
        dict_results = collections.Counter(np.array(clustering))  # return as dict
        results = np.array(
            list(dict_results.items()), dtype="float16"
        )  # store as numpy

        # inverse ratio: the small numbers become great and great numbers become small
        inverse_results = results.copy()
        sum = len(data)
        n = len(inverse_results[:, 1])
        if (n - 1) == 0:
            n = 2
        inverse_results[:, 1] = (1 - (inverse_results[:, 1] / sum)) / (n - 1)

        # get how many lines can get
        spots = np.ceil(
            np.array(inverse_results)[:, 1] * round_sample
        )  # calculating numbers of spot for each
        spots[spots < 1] = 1  # assume some groups are less involved

        # selecting index from the original line by the cluster groups
        for val, spot in zip(results[:, 0], spots):
            temp_indices = np.where(clustering == val)  # collecting index from copy
            if len(temp_indices[0]) < int(
                spot
            ):  # if spot is larger than num of group -> all
                spot = len(temp_indices[0])
            selected = random.sample(
                temp_indices[0].tolist(), k=int(spot)
            )  # randomly selecting

            for itr in selected:
                sample_index.append(np.where(line == shuffle_line[itr])[0][0])

        # this if for while loop and next round
        begin = end
        end += total_sample
        if end >= len(line):
            end = len(line)
        round += 1

    del shuffle_line  # to release the memory
    return sample_index
